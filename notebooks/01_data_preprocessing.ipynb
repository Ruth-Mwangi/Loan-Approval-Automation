{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moneza-Loan-Approval-Automation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook'\n",
    "\n",
    "# Import custom functions from the src folder\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "from visualization import create_bar_chart\n",
    "from utils import get_categorical_columns,get_numerical_columns,convert_to_data_type,get_boolean_columns\n",
    "\n",
    "from ydata_profiling import ProfileReport\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Load the Excel file\n",
    "file_path = \"../data/raw/TWINO_task_data_scientist.xlsx\"\n",
    "\n",
    "data = pd.read_excel(file_path,sheet_name=\"Data\")\n",
    "data.profile_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 3.1 Standardize Data Types and Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = get_categorical_columns(data)\n",
    "boolean_columns=get_boolean_columns(data)\n",
    "\n",
    "# Convert all text data to lowercase\n",
    "data[categorical_columns] = data[categorical_columns].apply(lambda x: x.str.lower().str.strip())\n",
    "data[categorical_columns] = convert_to_data_type(data,categorical_columns,\"category\")\n",
    "data[boolean_columns] = convert_to_data_type(data,boolean_columns,\"int\")\n",
    "data[boolean_columns] = convert_to_data_type(data,boolean_columns,\"category\")\n",
    "\n",
    "#Convert AR  to category\n",
    "data[\"AR\"] = convert_to_data_type(data,\"AR\",\"category\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Check for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing values for each column in the DataFrame\n",
    "null_percentage_columns = data.isna().mean() * 100\n",
    "null_percentage_columns = null_percentage_columns.sort_values(ascending=False)\n",
    "\n",
    "null_percentage_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Missing Data Findings\n",
    "- **Findings:** There are no missing values in any of the columns in the dataset.\n",
    "- **Action:** No action required as the dataset is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any duplicate rows\n",
    "duplicates = data.duplicated()\n",
    "duplicate_count = duplicates.sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    duplicate_rows = data[duplicates]\n",
    "    display(duplicate_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Duplicate Data Findings\n",
    "- **Findings:** There are no duplicate rows in the dataset.\n",
    "- **Action:** No action required as there are no duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Check for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop application_id column\n",
    "data = data.drop(\"application_id\", axis = 1)\n",
    "numerical_columns = get_numerical_columns(data)\n",
    "\n",
    "# Interquartile Range (IQR) method\n",
    "Q1 = data['warning_count'].quantile(0.25)\n",
    "Q3 = data['warning_count'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "iqr_outliers = data[(data['warning_count'] < lower_bound) | (data['warning_count'] > upper_bound)]\n",
    "print(\"Number of IQR outliers:\", len(iqr_outliers))\n",
    "\n",
    "# Z-Score method\n",
    "z_scores = stats.zscore(data['warning_count'])\n",
    "z_score_outliers = data[abs(z_scores) > 3]\n",
    "print(\"Number of Z-score outliers:\", len(z_score_outliers))\n",
    "\n",
    "# Combine outliers\n",
    "combined_outliers_index = iqr_outliers.index.union(z_score_outliers.index)\n",
    "num_combined_outliers = len(combined_outliers_index)\n",
    "print(\"Number of combined outliers:\", num_combined_outliers)\n",
    "combined_outliers = data.loc[combined_outliers_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_outliers_index = iqr_outliers.index.union(z_score_outliers.index)\n",
    "\n",
    "# Number of combined outliers\n",
    "num_combined_outliers = len(combined_outliers_index)\n",
    "print(\"Number of combined outliers:\", num_combined_outliers)\n",
    "\n",
    "# Extract the combined outliers\n",
    "combined_outliers = data.loc[combined_outliers_index]\n",
    "combined_outliers.warning_count.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data['warning_count'])\n",
    "plt.title('Warning Count')\n",
    "plt.ylabel('Values')\n",
    "\n",
    "# Annotate outliers with reduced arrow size\n",
    "for outlier in combined_outliers.itertuples():\n",
    "    plt.annotate(outlier.warning_count,\n",
    "                 xy=(0, outlier.warning_count),\n",
    "                 xytext=(0.05, outlier.warning_count + 0.2),\n",
    "                 textcoords='data',\n",
    "                 arrowprops=dict(facecolor='red', arrowstyle='-|>', lw=0.5),  # Adjust arrowstyle and lw (line width)\n",
    "                 fontsize=8, color='blue')\n",
    "\n",
    "plt.savefig(\"../reports/figures/warning_count_outliers.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bar_chart(combined_outliers,\"AR\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_outliers.warning_count.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outliers Findings\n",
    "- **Methodology:**\n",
    "  - **Interquartile Range (IQR):** Outliers are defined as data points outside 1.5 times the IQR above Q3 or below Q1. This method identified 24 outliers in the `warning_count` column.\n",
    "  - **Z-Score:** Outliers are defined as data points with a Z-score greater than 3 or less than -3. This method identified 52 outliers in the `warning_count` column.\n",
    "  - **Combined Approach:** Combining the outliers identified by both IQR and Z-score methods, a total of 52 unique outliers were observed.\n",
    "- **Findings:**\n",
    "  - 52 outliers were identified out of 9,898 records.\n",
    "  - I am going by the assumption that the outlier values (14, 13, 15, 16) are  reasonable and possible in real-life scenarios. Therefore, they are treated as edge cases rather than anomalies.\n",
    "- **Action:** The outliers will be retained in the dataset for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a colormap\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "\n",
    "royalblue = LinearSegmentedColormap.from_list('royalblue', [(0, (1,1,1)), (1, (0.25,0.41,0.88))])\n",
    "royalblue_r = royalblue.reversed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of the Spearman correlation\n",
    "import numpy as np\n",
    "from sklearn.calibration import LabelEncoder\n",
    "\n",
    "spearman=data.copy()\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# encode the categorical columns\n",
    "for category in get_categorical_columns(spearman):\n",
    "    if category !=\"payment_method\":\n",
    "        spearman[category]= label_encoder.fit_transform(spearman[category])\n",
    "spearman=pd.get_dummies(spearman,dtype='int64')\n",
    "\n",
    "target = 'AR'\n",
    "df_ordered = pd.concat([spearman.drop(target,axis=1), spearman[target]],axis=1)\n",
    "corr = df_ordered.corr(method='spearman')\n",
    "\n",
    "# Create a mask so that we see the correlation values only once\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask,1)] = True\n",
    "\n",
    "# Plot the heatmap correlation\n",
    "plt.figure(figsize=(30,30))\n",
    "sns.heatmap(corr, mask=mask, annot=True, cmap=royalblue, fmt='.2f', linewidths=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Save cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as parquet to persist data types\n",
    "data.to_parquet(\"../data/processed/data_cleaned.pqt\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
