{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan Approval Automation\n",
    "Table of Contents\n",
    "\n",
    "1. [Loan Approval Automation](#loan-approval-automation)\n",
    "   - [Executive Summary](#executive-summary)\n",
    "   - [Data Understanding and Preparation](#data-understanding-and-preparation)\n",
    "      - [Standardization of Data Types](#standardization-of-data-types)\n",
    "      - [Encoding Categorical Variables](#encoding-categorical-variables)\n",
    "      - [Missing Values Analysis](#missing-values-analysis)\n",
    "      - [Duplicate Rows Analysis](#duplicate-rows-analysis)\n",
    "      - [Outlier Detection Analysis](#outlier-detection-analysis)\n",
    "   - [Exploratory Data Analysis](#exploratory-data-analysis)\n",
    "      - [Univariate Analysis](#univariate-analysis)\n",
    "         - [Categorical Fields](#categorical-fields)\n",
    "         - [Boolean Data Check Fields](#boolean-data-check-fields)\n",
    "      - [Correlation Analysis](#correlation-analysis)\n",
    "   - [Models](#models)\n",
    "      - [Model Selection](#model-selection)\n",
    "      - [Model Training and Evaluation](#model-training-and-evaluation)\n",
    "      - [Probability Limits for Automated Decision Making](#probability-limits-for-automated-decision-making)\n",
    "         - [Optimal Model Thresholds](#optimal-model-thresholds)\n",
    "         - [Recommended Threshold](#recommended-threshold)\n",
    "   - [Business Impact Analysis](#business-impact-analysis)\n",
    "      - [Cost and Loss Comparison](#cost-and-loss-comparison)\n",
    "      - [Potential Savings](#potential-savings)\n",
    "   - [Conclusions](#conclusions)\n",
    "      - [Key Findings](#key-findings)\n",
    "      - [Improvements](#improvements)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary\n",
    "\n",
    "The objective of this project is to develop a machine learning model that will automate the rejection and approval of loan applications, with the aim of reducing labor costs and minimizing losses from incorrect decisions.\n",
    "\n",
    "### **Key Findings**\n",
    "- **Model Performance:** The Random Forest model achieved an accuracy of 84%.\n",
    "- **Probability Thresholds:** The recommended thresholds for auto-rejecting and auto-approving applications are set at 0.\n",
    "- **Cost Savings:** Implementing the model can save approximately 24,655 EUR, which is 50% of the total labor cost from manual processing\n",
    "\n",
    "These findings suggest that the model can significantly enhance the efficiency and accuracy of the loan approval process.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data Understanding and Preparation\n",
    "\n",
    "The dataset comprises 9898 records of loan applications that underwent manual approval. It includes the target variable (AR), which indicates whether an application should be accepted or rejected, alongside various characteristics of each application.The following steps were taken to ensure that the data is clean:\n",
    "\n",
    "- **Standardization of Data Types:** Ensured all data types were consistent and appropriate for analysis.\n",
    "- **Encoding Categorical Variables:** Applied one-hot encoding to transform categorical variables into a format suitable for machine learning algorithms.\n",
    "- **Missing Values Analysis:** No missing values were found, ensuring a complete dataset.\n",
    "- **Duplicate Rows Analysis:** The dataset contained no duplicate rows, affirming its uniqueness.\n",
    "- **Outlier Detection Analysis:** \n",
    "Outliers were detected in the warning count column.Some of the techniques used to detect outliers were:\n",
    "    - **Interquartile Range (IQR):** Identified 24 outliers.\n",
    "\n",
    "    - **Z-Score:** Identified 52 outliers.\n",
    "    \n",
    "    - **Combined Analysis:** A total of 52 unique outliers were observed by combining the IQR and Z-score methods. The outlier values ware assumed to be reasonable and possible in real-life scenarios. Therefore, they were treated as edge cases rather than anomalies and retained in the dataset.\n",
    "    \n",
    "    ![Warning Count Outliers](figures/warning_count_outliers.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "I conducted an exploratory data analysis (EDA) to understand the distribution and relationships within the cleaned dataset. \n",
    "\n",
    "### Univariate Analysis\n",
    "#### Categorical Fields\n",
    "Identified imbalance in the AR field, with 6,503 for the majority class and 3,395 for the minority class. Other features also showed imbalances. \n",
    "\n",
    "![Categorical unvariate analysis](figures/categorical_univariate_analysis.png)\n",
    "\n",
    "#### Boolean Data Check Fields\n",
    "Most fields related to data checks had an overwhelming amount of imbalance.\n",
    "\n",
    "**Recommendations for the data check fields:** Most fields have a minimal presence of `True` values, suggesting that they might not contribute significantly to the analysis.\n",
    "So focus should be on categories with a higher proportion of `True` values as they are more likely to provide relevant and actionable insights. Fields with an overwhelming majority of `False` values and do not have a strong relationship with the AR column will be dropped, as they may not add substantial value to the analysis.\n",
    "\n",
    "![Data Checks univariate analysis](figures/data_checks_univariate_analysis.png)\n",
    "\n",
    "### Correlation Analysis\n",
    "\n",
    "An analysis of of how the feature interact with each other and most importantly how they interact with the target was done. Features that had multicollinearity were either:\n",
    "- Combined features to enhance positive relationships with the target.\n",
    "- Removed less informative features.\n",
    "\n",
    "#### Findings and  Recommendations:\n",
    "\n",
    "- **Columns with weak relationship with the target :**\n",
    "   - **Columns with Coefficients â‰¤ 0:** Since these variables show either a very weak or negative relationship with AR, they may add noise or complexity to the model without providing meaningful predictive power. Removing these columns could simplify the model and potentially improve its performance.\n",
    "\n",
    "- **Columns with stron realtionship with the target:**\n",
    "   - **Columns with Positive Coefficients:** These variables show a more meaningful positive correlation with the AR and are likely to provide useful information for predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "### Model Selection\n",
    "The following models were tested.\n",
    "\n",
    "- **Logistic regression**\n",
    "- **Random Forests**\n",
    "- **XGBoost**\n",
    "- **An Ensemble of the three**\n",
    "\n",
    "#### Variables Used\n",
    "- client_first_lapp_mark\n",
    "- client_first_manual_lapp_mark\n",
    "- warning_count\n",
    "- data_comparison_8\n",
    "- data_comparison_13\n",
    "- creditcard_check_16\n",
    "- data_quality_19\n",
    "- data_quality_24\n",
    "- data_quality_27\n",
    "- data_quality_28\n",
    "- data_quality_32\n",
    "- data_quality_33\n",
    "- accounts_check_38\n",
    "- accounts_check_sum - `An engineered field for all account check fields`\n",
    "- creditcard_check_sum - `An engineered field for all credit card check fields`\n",
    "- data_comparison_check_sum - `An engineered field for all data comparison check fields`\n",
    "- data_quality_check_sum - `An engineered field for all data quality check fields`\n",
    "- other_check_sum - `An engineered field for all other check fields`\n",
    "- payment_method\n",
    "- warning_count&data_comparison_15 - `An engineered field from multicollinear field increased relationship with target`\n",
    "- warning_count&data_comparison_14 - `An engineered field from multicollinear field increased relationship with target`\n",
    "- warning_count&data_comparison_12 - `An engineered field from multicollinear field increased relationship with target`\n",
    "- warning_count&data_comparison_11 - `An engineered field from multicollinear field increased relationship with target`\n",
    "\n",
    "\n",
    "\n",
    "### Model Training and Evaluation\n",
    "The data was split into training (80%) and testing (20%) sets. SMOTE resampling techniques were applied to address class imbalance. GridSearchCV and RandomizedSearchCV for hyperparameter tuning of the models.\n",
    "\n",
    "#### Model Performance \n",
    "To assess and compare the precision of the models, I used the following metrics.\n",
    "\n",
    "\n",
    "| Model                | Accuracy  | Precision | Recall    | F1-score  |\n",
    "|----------------------|-----------|-----------|-----------|-----------|\n",
    "| Random Forest        | 8-10%    | 79.34%    | 7-52%    | 7-78%    |\n",
    "| XGBoost              | 7-02%    | 60.91%    | 5-40%    | 5-75%    |\n",
    "| Logistic Regression  | 7-47%    | 6-80%    | 5-69%    | 5-30%    |\n",
    "| Ensemble             | 7-54%    | 6-25%    | 5-49%    | 58.54%    |\n",
    "\n",
    "The table above shows the performance metrics for four different models: Random Forest, XGBoost, Logistic Regression, and an Ensemble model. The metrics include Accuracy, Precision, Recall, and F1-score, which are essential for evaluating the effectiveness of a classifier.\n",
    "\n",
    "Among these models, the Random Forest classifier exhibited the highest performance across most metrics, achieving an accuracy of 8-10%, a precision of 79.34%, a recall of 7-52%, and an F1-score of 7-78%. These metrics indicate that the Random Forest model is the most effective in balancing precision and recall, leading to a higher overall performance.\n",
    "\n",
    "Therefore, we chose the Random Forest classifier due to its superior performance in terms of accuracy, precision, recall, and F1-score compared to the other models. This makes it the most reliable choice for our classification task.\n",
    "\n",
    "\n",
    "![ROC of models](figures/roc.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Limits for Automated Decision Making\n",
    "Evaluated multiple thresholds from 0.1 - 0.9 by classifying data based on predicted probabilities of various models, and calculating the costs of false negatives and false positives. The threshold that results in the lowest total cost of loss and provides the optimal balance between false negatives and false positives was selected. \n",
    "\n",
    "### Optimal Model Thresholds\n",
    "Below is a list of optimal thresholds for the models used:\n",
    "\n",
    "\n",
    "| Model Name         | Threshold | Cost   | False Negative Count | False Negative Cost | False Positive Count | False Positive Cost |\n",
    "|--------------------|-----------|--------|-----------------------|----------------------|-----------------------|----------------------|\n",
    "| Random Forest      | 0.50      | 24,835 | 933                   | 15,861               | 641                   | 8,974                |\n",
    "| XGBoost            | 0.51      | 37,337 | 1,465                 | 24,905               | 888                   | 12,432               |\n",
    "| Logistic Regression| 0.51      | 43,367 | 1,711                 | 29,087               | 1,020                 | 14,280               |\n",
    "| Ensemble           | 0.50      | 32,892 | 1,304                 | 22,168               | 766                   | 10,724               |\n",
    "\n",
    "![Model Threshold](figures/model_optimum_thresholds.png)\n",
    "\n",
    "### Recommended Threshold\n",
    "Based on the above , I recommend using the  Random Forest model :\n",
    "- Auto-reject applications with probability >= 0.5\n",
    "- Auto-approve applications with probability < 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Impact Analysis\n",
    "\n",
    "### Cost and Loss Comparison\n",
    "The Cost and Loss comparison is based on 9898 loan applications.\n",
    "\n",
    "#### Without Model\n",
    "- Total labor cost: 49,490\n",
    "- Total loss from incorrect decisions: 0\n",
    "\n",
    "#### With Model\n",
    "Out of the 9,898 loans 1,574 loans were incorrectly labled, 641 loans that should have been approved were rejected and 933 loans that should have been rejected were approved. \n",
    "With the model:\n",
    "- Total labor cost: 0\n",
    "- Total loss from incorrect decisions: 24,835\n",
    "\n",
    "#### Potential Savings\n",
    "Implementing the model saves 24,655 EUR, which is 50% of the labor cost, with potential for further improvement.\n",
    "\n",
    "|                       | **Without Model** | **With Model** |\n",
    "|-----------------------|-------------------|----------------|\n",
    "| **Total Labor Cost**  | 49,490 EUR        | 0 EUR          |\n",
    "| **Total Loss from Incorrect Decisions** | 0 EUR            | 24,835 EUR     |\n",
    "\n",
    "![Cost and Loss Comparison](figures/cost_loss.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "### Key Findings\n",
    "- The machine learning model can effectively automate the loan approval process, achieving an accuracy of 8-10% with the Random Forest classifier. This reduces manual processing costs and minimizes losses from incorrect decisions.\n",
    "- The optimal threshold for the Random Forest model is 0.5, leading to the best balance between false positives and false negatives and overall reduction of loss .\n",
    "\n",
    "### Improvements\n",
    "\n",
    "- **Feature Engineering:** Explore additional feature engineering techniques to enhance the model's predictive power. This could involve creating new features or improving existing ones based on domain knowledge and data insights.\n",
    "- **Explore Different Models:** While the Random Forest model has shown the best performance among the tested models, exploring other machine learning models and techniques should be done to ensure the highest possible accuracy and reliability from the model.\n",
    "\n",
    "\n",
    "By implementing these recommendations, the company can significantly enhance the efficiency and accuracy of the loan approval process, leading to substantial cost savings and improved decision-making. Continuous monitoring and iterative improvements will ensure the model remains robust and effective over time.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
